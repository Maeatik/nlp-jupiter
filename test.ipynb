{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/marat/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/marat/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Загрузка необходимых данных для NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка файла CSV\n",
    "file_path = \"mc.csv\"  # Замените на путь к вашему файлу\n",
    "data = pd.read_csv(file_path, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удаление дубликатов\n",
    "data.drop_duplicates(inplace=True)\n",
    "\n",
    "# Удаление строк с пропущенными значениями в колонке Comment\n",
    "data = data.dropna(subset=[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Очистка текста: удаление ссылок, эмодзи, лишних пробелов\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE)  # Удаление ссылок\n",
    "    text = re.sub(r\"[^a-zA-Zа-яА-Я0-9\\s]\", '', text)  # Удаление всех символов кроме букв и цифр\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Удаление лишних пробелов\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"cleaned_review\"] = data[\"review\"].apply(clean_text)\n",
    "\n",
    "# Нормализация: перевод текста в нижний регистр\n",
    "data[\"cleaned_review\"] = data[\"cleaned_review\"].str.lower()\n",
    "\n",
    "# Токенизация\n",
    "data[\"tokens\"] = data[\"cleaned_review\"].apply(word_tokenize)\n",
    "\n",
    "# Удаление стоп-слов (по желанию)\n",
    "stop_words = set(stopwords.words(\"russian\") + stopwords.words(\"english\"))  # Добавьте языки по необходимости\n",
    "\n",
    "data[\"tokens_no_stopwords\"] = data[\"tokens\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Предобработка завершена. Файл сохранен: processed_data2.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Сохранение результата в новый файл\n",
    "output_file = \"processed_data2.xlsx\"\n",
    "data.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Предобработка завершена. Файл сохранен: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF векторизация завершена. Размерность: (33396, 1000)\n"
     ]
    }
   ],
   "source": [
    "# 1. TF-IDF\n",
    "def apply_tfidf(data):\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)  # Максимум 1000 признаков для примера\n",
    "    tfidf_matrix = vectorizer.fit_transform(data[\"cleaned_review\"]).toarray()\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    return tfidf_matrix, feature_names\n",
    "\n",
    "tfidf_matrix, tfidf_features = apply_tfidf(data)\n",
    "print(f\"TF-IDF векторизация завершена. Размерность: {tfidf_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec векторизация завершена. Размерность: (33396, 100)\n"
     ]
    }
   ],
   "source": [
    "# 2. Word2Vec\n",
    "def train_word2vec(tokens):\n",
    "    model = Word2Vec(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "word2vec_model = train_word2vec(data[\"tokens\"])\n",
    "word2vec_vectors = np.array([\n",
    "    np.mean([word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv], axis=0)\n",
    "    if len(tokens) > 0 else np.zeros(word2vec_model.vector_size)\n",
    "    for tokens in data[\"tokens\"]\n",
    "])\n",
    "print(f\"Word2Vec векторизация завершена. Размерность: {word2vec_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText векторизация завершена. Размерность: (33396, 100)\n"
     ]
    }
   ],
   "source": [
    "# 3. FastText\n",
    "def train_fasttext(tokens):\n",
    "    model = FastText(sentences=tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "fasttext_model = train_fasttext(data[\"tokens\"])\n",
    "fasttext_vectors = np.array([\n",
    "    np.mean([fasttext_model.wv[word] for word in tokens if word in fasttext_model.wv], axis=0)\n",
    "    if len(tokens) > 0 else np.zeros(fasttext_model.vector_size)\n",
    "    for tokens in data[\"tokens\"]\n",
    "])\n",
    "print(f\"FastText векторизация завершена. Размерность: {fasttext_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec векторизация завершена. Размерность: (33396, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 2. Word2Vec\n",
    "def train_vectorizer(tokens):\n",
    "    vectorizer = CountVectorizer(max_features=1000)  # Максимум 1000 признаков\n",
    "    X_counts = vectorizer.fit_transform(data[\"cleaned_review\"])\n",
    "    return X_counts.toarray()\n",
    "\n",
    "vectorizer_vectors  = train_vectorizer(data[\"tokens\"]) \n",
    "print(f\"Word2Vec векторизация завершена. Размерность: {vectorizer_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Сохранение результатов для дальнейшей работы\n",
    "np.save(\"tfidf_vectors.npy\", tfidf_matrix)\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.24798714,  0.15006742, -0.31912231, ..., -0.5464887 ,\n",
       "         0.0266407 , -0.23988548],\n",
       "       [ 0.31747019,  0.05300501,  0.16994421, ..., -0.22851613,\n",
       "        -0.17921297, -0.03659187],\n",
       "       [-0.02589575,  0.35349786, -0.39811826, ..., -0.82562613,\n",
       "         0.22126336, -0.54033977],\n",
       "       ...,\n",
       "       [ 0.51317132, -0.0359251 ,  0.0462436 , ..., -0.20405555,\n",
       "         0.11780679, -0.45187187],\n",
       "       [ 0.28966463, -0.1966936 ,  0.2243028 , ...,  0.19946809,\n",
       "        -0.32800543,  0.01052366],\n",
       "       [ 0.25528821, -0.37508866,  0.44096556, ..., -0.69754839,\n",
       "         0.67834592, -0.55464244]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\"word2vec_vectors.npy\", word2vec_vectors)\n",
    "word2vec_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.10970908, -0.35354751, -0.57972389, ..., -0.12764983,\n",
       "         0.43826881,  0.11531623],\n",
       "       [-0.4964397 , -0.02631063, -0.27072978, ..., -0.4892619 ,\n",
       "         0.06523692,  0.36658874],\n",
       "       [ 0.18118294, -0.29088449, -0.51115525, ..., -0.49640018,\n",
       "        -0.2248057 ,  0.10768735],\n",
       "       ...,\n",
       "       [-0.15396152,  0.28586802, -0.06608979, ...,  0.22940528,\n",
       "         0.5257901 ,  0.11973723],\n",
       "       [-0.3999916 , -0.35895768,  0.16134413, ..., -0.23990907,\n",
       "         0.07201809,  0.13066594],\n",
       "       [-0.90981585, -0.7439577 , -0.22964275, ..., -0.1411529 ,\n",
       "         0.43326142,  0.61729252]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\"fasttext_vectors.npy\", fasttext_vectors)\n",
    "fasttext_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(\"vectorizer_vectors.npy\", vectorizer_vectors)\n",
    "vectorizer_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = tfidf_matrix  # Используем TF-IDF как векторизацию (можно заменить на Word2Vec или FastText)\n",
    "# vectors = word2vec_vectors\n",
    "# vectors = fasttext_vectors\n",
    "vectors = vectorizer_vectors\n",
    "\n",
    "labels = data[\"rating\"]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Функция для обучения и оценки модели\n",
    "def train_and_evaluate(model, vector, name):\n",
    "\n",
    "    # Разделение на обучающую и тестовую выборки\n",
    "    X_train, X_test, y_train, y_test = train_test_split(vector, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "\n",
    "    # Проверка на наличие отрицательных значений\n",
    "    print(\"Min value in X_train:\", X_train.min())\n",
    "    print(\"Min value in X_test:\", X_test.min())\n",
    "\n",
    "    # Проверка на наличие отрицательных значений\n",
    "    print(\"Max value in X_train:\", X_train.max())\n",
    "    print(\"Max value in X_test:\", X_test.max())\n",
    "\n",
    "    print(\"Len value in X_train:\", len(X_train))\n",
    "    print(\"Len value in X_test:\", len(X_test))\n",
    "\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Метрики\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value in X_train: 0.0\n",
      "Min value in X_test: 0.0\n",
      "Max value in X_train: 1.0\n",
      "Max value in X_test: 1.0\n",
      "Len value in X_train: 26716\n",
      "Len value in X_test: 6680\n",
      "\n",
      "=== Наивный Байес ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.60      0.91      0.72      1886\n",
      "     2 stars       0.78      0.12      0.20       617\n",
      "     3 stars       0.64      0.32      0.43       964\n",
      "     4 stars       0.60      0.37      0.46      1158\n",
      "     5 stars       0.66      0.80      0.73      2055\n",
      "\n",
      "    accuracy                           0.63      6680\n",
      "   macro avg       0.66      0.51      0.51      6680\n",
      "weighted avg       0.64      0.63      0.59      6680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1720   17   29   20  100]\n",
      " [ 416   71   39   29   62]\n",
      " [ 356    2  313   99  194]\n",
      " [ 174    1   66  433  484]\n",
      " [ 222    0   39  146 1648]]\n"
     ]
    }
   ],
   "source": [
    "# 1. Наивный байесовский классификатор\n",
    "nb_model = MultinomialNB()\n",
    "results.append(train_and_evaluate(nb_model, tfidf_matrix, \"Наивный Байес\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value in X_train: -3.240269899368286\n",
      "Min value in X_test: -3.091440200805664\n",
      "Max value in X_train: 4.25397253036499\n",
      "Max value in X_test: 4.25397253036499\n",
      "Len value in X_train: 26716\n",
      "Len value in X_test: 6680\n",
      "\n",
      "=== Логистическая регрессия ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.61      0.88      0.72      1886\n",
      "     2 stars       0.63      0.13      0.21       617\n",
      "     3 stars       0.49      0.37      0.42       964\n",
      "     4 stars       0.61      0.35      0.44      1158\n",
      "     5 stars       0.65      0.75      0.70      2055\n",
      "\n",
      "    accuracy                           0.61      6680\n",
      "   macro avg       0.59      0.50      0.50      6680\n",
      "weighted avg       0.60      0.61      0.57      6680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1665   17  101   14   89]\n",
      " [ 395   79   56   22   65]\n",
      " [ 302   18  355   76  213]\n",
      " [ 156    7  105  407  483]\n",
      " [ 233    5  114  153 1550]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# 2. Логистическая регрессия\n",
    "lr_model = LogisticRegression()\n",
    "results.append(train_and_evaluate(lr_model, word2vec_vectors, \"Логистическая регрессия\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value in X_train: -8.752981185913086\n",
      "Min value in X_test: -3.74164080619812\n",
      "Max value in X_train: 6.57462739944458\n",
      "Max value in X_test: 3.836000919342041\n",
      "Len value in X_train: 26716\n",
      "Len value in X_test: 6680\n",
      "\n",
      "=== SVM ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.59      0.90      0.71      1886\n",
      "     2 stars       0.97      0.10      0.19       617\n",
      "     3 stars       0.55      0.33      0.42       964\n",
      "     4 stars       0.77      0.27      0.40      1158\n",
      "     5 stars       0.61      0.82      0.70      2055\n",
      "\n",
      "    accuracy                           0.61      6680\n",
      "   macro avg       0.70      0.49      0.48      6680\n",
      "weighted avg       0.66      0.61      0.56      6680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1690    1   57    3  135]\n",
      " [ 410   64   48   10   85]\n",
      " [ 330    1  320   30  283]\n",
      " [ 178    0   93  315  572]\n",
      " [ 254    0   60   51 1690]]\n"
     ]
    }
   ],
   "source": [
    "# 3. Метод опорных векторов (SVM)\n",
    "svm_model = SVC()\n",
    "results.append(train_and_evaluate(svm_model, fasttext_vectors, \"SVM\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value in X_train: 0\n",
      "Min value in X_test: 0\n",
      "Max value in X_train: 37\n",
      "Max value in X_test: 37\n",
      "Len value in X_train: 26716\n",
      "Len value in X_test: 6680\n",
      "\n",
      "=== Дерево решений ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.74      0.74      0.74      1886\n",
      "     2 stars       0.47      0.44      0.46       617\n",
      "     3 stars       0.50      0.46      0.48       964\n",
      "     4 stars       0.54      0.52      0.53      1158\n",
      "     5 stars       0.69      0.74      0.72      2055\n",
      "\n",
      "    accuracy                           0.63      6680\n",
      "   macro avg       0.59      0.58      0.58      6680\n",
      "weighted avg       0.63      0.63      0.63      6680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1388  174  127   61  136]\n",
      " [ 181  274   67   46   49]\n",
      " [ 158   67  448  132  159]\n",
      " [  80   26  121  604  327]\n",
      " [  76   42  142  274 1521]]\n"
     ]
    }
   ],
   "source": [
    "# 4. Дерево решений\n",
    "dt_model = DecisionTreeClassifier()\n",
    "results.append(train_and_evaluate(dt_model, vectorizer_vectors, \"Дерево решений\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer_vectors\n",
    "\n",
    "labels = data[\"rating\"] \n",
    "mapped_labels = labels.map({'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4})\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectors, mapped_labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "# Нормализация данных для MultinomialNB\n",
    "\n",
    "def train_and_evaluateBoosting(model, name):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=1)\n",
    "\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "    return {\n",
    "        \"model\": name,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(mapped_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [13:13:08] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== XGBoost ===\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.87      0.77      1886\n",
      "           1       0.63      0.25      0.36       617\n",
      "           2       0.62      0.41      0.50       964\n",
      "           3       0.60      0.44      0.50      1158\n",
      "           4       0.67      0.83      0.74      2055\n",
      "\n",
      "    accuracy                           0.66      6680\n",
      "   macro avg       0.64      0.56      0.57      6680\n",
      "weighted avg       0.65      0.66      0.64      6680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1633   55   39   30  129]\n",
      " [ 300  156   50   34   77]\n",
      " [ 246   24  399   90  205]\n",
      " [ 105   10   90  505  448]\n",
      " [  90    4   63  188 1710]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Бустинг (XGBoost)\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42)\n",
    "results.append(train_and_evaluateBoosting(xgb_model, \"XGBoost\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     model  accuracy  precision    recall        f1\n",
      "0            Наивный Байес  0.626497   0.640176  0.626497  0.587104\n",
      "1  Логистическая регрессия  0.607186   0.602546  0.607186  0.574022\n",
      "2                      SVM  0.610629   0.657721  0.610629  0.563617\n",
      "3           Дерево решений  0.633982   0.630165  0.633982  0.631711\n",
      "4                  XGBoost  0.659132   0.650033  0.659132  0.636343\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Вывод результатов\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "ARI (KMeans): 0.0809\n",
      "Silhouette Score: 0.1675\n",
      "Davies-Bouldin Index: 1.9217\n",
      "Calinski-Harabasz Index: 6435.8273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score \n",
    "\n",
    "vectors = word2vec_vectors\n",
    "n_clusters = 5  # Количество кластеров\n",
    "\n",
    "# Применение K-средних\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(vectors)\n",
    "\n",
    "print(1)\n",
    "# Оценка качества кластеризации\n",
    "silhouette_avg_k = silhouette_score(vectors, kmeans_labels)\n",
    "davies_bouldin_k = davies_bouldin_score(vectors, kmeans_labels)\n",
    "calinski_harabasz_k = calinski_harabasz_score(vectors, kmeans_labels)\n",
    "\n",
    "\n",
    "if 'labels' in locals():\n",
    "    ari = adjusted_rand_score(labels, kmeans_labels)\n",
    "    print(f\"ARI (KMeans): {ari:.4f}\")\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg_k:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_k:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.1620\n",
      "Davies-Bouldin Index: 3.2858\n",
      "Calinski-Harabasz Index: 4408.8550\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Подбор количества кластеров\n",
    "n_clusters = 5  # Задайте желаемое количество кластеров\n",
    "\n",
    "# Обучение GMM\n",
    "gmm = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)\n",
    "gmm_labels = gmm.fit_predict(vectors)\n",
    "\n",
    "# Оценка качества кластеризации\n",
    "silhouette_avg_g = silhouette_score(vectors, gmm_labels)\n",
    "davies_bouldin_g = davies_bouldin_score(vectors, gmm_labels)\n",
    "calinski_harabasz_g = calinski_harabasz_score(vectors, gmm_labels)\n",
    "\n",
    "print(f\"Silhouette Score: {silhouette_avg_g:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_g:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz_g:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score (Hierarchical): 0.1229\n",
      "Davies-Bouldin Index: 1.8923\n",
      "Calinski-Harabasz Index: 6174.9417\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Применение иерархической кластеризации\n",
    "linkage_matrix = linkage(vectors, method='ward')\n",
    "\n",
    "# Выделение кластеров\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "hierarchical_labels = hierarchical.fit_predict(vectors)\n",
    "\n",
    "# Метрика силуэт\n",
    "silhouette_avg_h = silhouette_score(vectors, hierarchical_labels)\n",
    "davies_bouldin_h = davies_bouldin_score(vectors, hierarchical_labels)\n",
    "calinski_harabasz_h = calinski_harabasz_score(vectors, hierarchical_labels)\n",
    "\n",
    "print(f\"Silhouette Score (Hierarchical): {silhouette_avg_h:.4f}\")\n",
    "print(f\"Davies-Bouldin Index: {davies_bouldin_h:.4f}\")\n",
    "print(f\"Calinski-Harabasz Index: {calinski_harabasz_h:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 GMM  AgglomerativeClustering      K-Means\n",
      "Silhouette Score            0.161985                 0.122929     0.167490\n",
      "Davies-Bouldin Index        3.285804                 1.892295     1.921726\n",
      "Calinski-Harabasz Index  4408.854991              6174.941662  6435.827253\n"
     ]
    }
   ],
   "source": [
    "dataClustering = {\n",
    "    \"GMM\": [silhouette_avg_g, davies_bouldin_g, calinski_harabasz_g],\n",
    "    \"AgglomerativeClustering\": [silhouette_avg_h, davies_bouldin_h, calinski_harabasz_h],\n",
    "    \"K-Means\": [silhouette_avg_k, davies_bouldin_k, calinski_harabasz_k]\n",
    "}\n",
    "\n",
    "# Индексы для строк\n",
    "metrics = [\"Silhouette Score\", \"Davies-Bouldin Index\", \"Calinski-Harabasz Index\"]\n",
    "\n",
    "# Создание DataFrame\n",
    "dfClustering = pd.DataFrame(dataClustering, index=metrics)\n",
    "\n",
    "# Вывод таблицы\n",
    "print(dfClustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>category</th>\n",
       "      <th>store_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>why does it look like someone spit on my food ...</td>\n",
       "      <td>[why, does, it, look, like, someone, spit, on,...</td>\n",
       "      <td>[look, like, someone, spit, food, normal, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>itd mcdonalds it is what it is as far as the f...</td>\n",
       "      <td>[itd, mcdonalds, it, is, what, it, is, as, far...</td>\n",
       "      <td>[itd, mcdonalds, far, food, atmosphere, go, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>made a mobile order got to the speaker and che...</td>\n",
       "      <td>[made, a, mobile, order, got, to, the, speaker...</td>\n",
       "      <td>[made, mobile, order, got, speaker, checked, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>my mc crispy chicken sandwich was customer ser...</td>\n",
       "      <td>[my, mc, crispy, chicken, sandwich, was, custo...</td>\n",
       "      <td>[mc, crispy, chicken, sandwich, customer, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>i repeat my order 3 times in the drive thru an...</td>\n",
       "      <td>[i, repeat, my, order, 3, times, in, the, driv...</td>\n",
       "      <td>[repeat, order, 3, times, drive, thru, still, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33391</th>\n",
       "      <td>33392</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>They treated me very badly.</td>\n",
       "      <td>1 star</td>\n",
       "      <td>they treated me very badly</td>\n",
       "      <td>[they, treated, me, very, badly]</td>\n",
       "      <td>[treated, badly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33392</th>\n",
       "      <td>33393</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>The service is very good</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>the service is very good</td>\n",
       "      <td>[the, service, is, very, good]</td>\n",
       "      <td>[service, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33393</th>\n",
       "      <td>33394</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>To remove hunger is enough</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>to remove hunger is enough</td>\n",
       "      <td>[to, remove, hunger, is, enough]</td>\n",
       "      <td>[remove, hunger, enough]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33394</th>\n",
       "      <td>33395</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>It's good, but lately it has become very expen...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>its good but lately it has become very expensive</td>\n",
       "      <td>[its, good, but, lately, it, has, become, very...</td>\n",
       "      <td>[good, lately, become, expensive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33395</th>\n",
       "      <td>33396</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>they took good care of me</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>they took good care of me</td>\n",
       "      <td>[they, took, good, care, of, me]</td>\n",
       "      <td>[took, good, care]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33396 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewer_id  store_name              category  \\\n",
       "0                1  McDonald's  Fast food restaurant   \n",
       "1                2  McDonald's  Fast food restaurant   \n",
       "2                3  McDonald's  Fast food restaurant   \n",
       "3                4  McDonald's  Fast food restaurant   \n",
       "4                5  McDonald's  Fast food restaurant   \n",
       "...            ...         ...                   ...   \n",
       "33391        33392  McDonald's  Fast food restaurant   \n",
       "33392        33393  McDonald's  Fast food restaurant   \n",
       "33393        33394  McDonald's  Fast food restaurant   \n",
       "33394        33395  McDonald's  Fast food restaurant   \n",
       "33395        33396  McDonald's  Fast food restaurant   \n",
       "\n",
       "                                           store_address  latitude   \\\n",
       "0      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "1      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "2      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "3      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "4      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "...                                                  ...        ...   \n",
       "33391  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33392  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33393  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33394  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33395  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "\n",
       "       longitude rating_count   review_time  \\\n",
       "0     -97.792874        1,240  3 months ago   \n",
       "1     -97.792874        1,240    5 days ago   \n",
       "2     -97.792874        1,240    5 days ago   \n",
       "3     -97.792874        1,240   a month ago   \n",
       "4     -97.792874        1,240  2 months ago   \n",
       "...          ...          ...           ...   \n",
       "33391 -80.189098        2,810   4 years ago   \n",
       "33392 -80.189098        2,810    a year ago   \n",
       "33393 -80.189098        2,810    a year ago   \n",
       "33394 -80.189098        2,810   5 years ago   \n",
       "33395 -80.189098        2,810   2 years ago   \n",
       "\n",
       "                                                  review   rating  \\\n",
       "0      Why does it look like someone spit on my food?...   1 star   \n",
       "1      It'd McDonalds. It is what it is as far as the...  4 stars   \n",
       "2      Made a mobile order got to the speaker and che...   1 star   \n",
       "3      My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...  5 stars   \n",
       "4      I repeat my order 3 times in the drive thru, a...   1 star   \n",
       "...                                                  ...      ...   \n",
       "33391                        They treated me very badly.   1 star   \n",
       "33392                           The service is very good  5 stars   \n",
       "33393                         To remove hunger is enough  4 stars   \n",
       "33394  It's good, but lately it has become very expen...  5 stars   \n",
       "33395                          they took good care of me  5 stars   \n",
       "\n",
       "                                          cleaned_review  \\\n",
       "0      why does it look like someone spit on my food ...   \n",
       "1      itd mcdonalds it is what it is as far as the f...   \n",
       "2      made a mobile order got to the speaker and che...   \n",
       "3      my mc crispy chicken sandwich was customer ser...   \n",
       "4      i repeat my order 3 times in the drive thru an...   \n",
       "...                                                  ...   \n",
       "33391                         they treated me very badly   \n",
       "33392                           the service is very good   \n",
       "33393                         to remove hunger is enough   \n",
       "33394   its good but lately it has become very expensive   \n",
       "33395                          they took good care of me   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [why, does, it, look, like, someone, spit, on,...   \n",
       "1      [itd, mcdonalds, it, is, what, it, is, as, far...   \n",
       "2      [made, a, mobile, order, got, to, the, speaker...   \n",
       "3      [my, mc, crispy, chicken, sandwich, was, custo...   \n",
       "4      [i, repeat, my, order, 3, times, in, the, driv...   \n",
       "...                                                  ...   \n",
       "33391                   [they, treated, me, very, badly]   \n",
       "33392                     [the, service, is, very, good]   \n",
       "33393                   [to, remove, hunger, is, enough]   \n",
       "33394  [its, good, but, lately, it, has, become, very...   \n",
       "33395                   [they, took, good, care, of, me]   \n",
       "\n",
       "                                     tokens_no_stopwords  \n",
       "0      [look, like, someone, spit, food, normal, tran...  \n",
       "1      [itd, mcdonalds, far, food, atmosphere, go, st...  \n",
       "2      [made, mobile, order, got, speaker, checked, l...  \n",
       "3      [mc, crispy, chicken, sandwich, customer, serv...  \n",
       "4      [repeat, order, 3, times, drive, thru, still, ...  \n",
       "...                                                  ...  \n",
       "33391                                   [treated, badly]  \n",
       "33392                                    [service, good]  \n",
       "33393                           [remove, hunger, enough]  \n",
       "33394                  [good, lately, become, expensive]  \n",
       "33395                                 [took, good, care]  \n",
       "\n",
       "[33396 rows x 13 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = word2vec_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokens = data[\"tokens_no_stopwords\"]\n",
    "labels = data[\"rating\"] \n",
    "y = labels.map({'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4})\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=5)\n",
    "\n",
    "X = vectors\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.5),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # 5 классов\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Определение модели GRU\n",
    "def create_gru_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.GRU(64, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Определение модели BiRNN\n",
    "def create_birnn_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(5, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 22ms/step - accuracy: 0.4386 - loss: 1.3520 - val_accuracy: 0.5006 - val_loss: 1.2553\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.4925 - loss: 1.2470 - val_accuracy: 0.5027 - val_loss: 1.2443\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.4953 - loss: 1.2426 - val_accuracy: 0.5075 - val_loss: 1.2377\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.4999 - loss: 1.2287 - val_accuracy: 0.5034 - val_loss: 1.2419\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 19ms/step - accuracy: 0.5035 - loss: 1.2216 - val_accuracy: 0.5058 - val_loss: 1.2399\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5024 - loss: 1.2453\n",
      "Test accuracy: 50.75%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training LSTM model...\n",
      "Epoch 1/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 26ms/step - accuracy: 0.3526 - loss: 1.5015 - val_accuracy: 0.4431 - val_loss: 1.4505\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.4242 - loss: 1.3846 - val_accuracy: 0.4428 - val_loss: 1.3708\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.4486 - loss: 1.3599 - val_accuracy: 0.4569 - val_loss: 1.3827\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 24ms/step - accuracy: 0.4613 - loss: 1.3412 - val_accuracy: 0.4795 - val_loss: 1.3051\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.4645 - loss: 1.3196 - val_accuracy: 0.4593 - val_loss: 1.3185\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4716 - loss: 1.3111\n",
      "Test accuracy: 47.95%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Training GRU model...\n",
      "Epoch 1/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 27ms/step - accuracy: 0.3414 - loss: 1.5134 - val_accuracy: 0.4168 - val_loss: 1.4788\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.3951 - loss: 1.4465 - val_accuracy: 0.4177 - val_loss: 1.3985\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.4099 - loss: 1.4107 - val_accuracy: 0.4249 - val_loss: 1.4197\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.4321 - loss: 1.3901 - val_accuracy: 0.4534 - val_loss: 1.3705\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 32ms/step - accuracy: 0.4386 - loss: 1.3784 - val_accuracy: 0.4220 - val_loss: 1.3892\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4521 - loss: 1.3712\n",
      "Test accuracy: 45.34%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Training BiRNN model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/marat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/marat/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 37ms/step - accuracy: 0.3606 - loss: 1.4882 - val_accuracy: 0.4660 - val_loss: 1.4297\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 32ms/step - accuracy: 0.4516 - loss: 1.3450 - val_accuracy: 0.4853 - val_loss: 1.3097\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.4671 - loss: 1.3131 - val_accuracy: 0.4901 - val_loss: 1.2833\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 34ms/step - accuracy: 0.4803 - loss: 1.2934 - val_accuracy: 0.4829 - val_loss: 1.2786\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.4757 - loss: 1.2967 - val_accuracy: 0.4856 - val_loss: 1.2825\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.4803 - loss: 1.2829\n",
      "Test accuracy: 48.29%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\n",
      "CNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.42      0.94      0.58      1898\n",
      "     2 stars       0.92      0.09      0.17       645\n",
      "     3 stars       0.44      0.05      0.09       939\n",
      "     4 stars       0.85      0.25      0.38      1146\n",
      "     5 stars       0.65      0.59      0.62      2052\n",
      "\n",
      "    accuracy                           0.51      6680\n",
      "   macro avg       0.65      0.38      0.37      6680\n",
      "weighted avg       0.61      0.51      0.45      6680\n",
      "\n",
      "\n",
      "LSTM Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.40      0.90      0.55      1898\n",
      "     2 stars       0.72      0.09      0.16       645\n",
      "     3 stars       0.33      0.00      0.01       939\n",
      "     4 stars       0.81      0.25      0.38      1146\n",
      "     5 stars       0.59      0.56      0.57      2052\n",
      "\n",
      "    accuracy                           0.48      6680\n",
      "   macro avg       0.57      0.36      0.34      6680\n",
      "weighted avg       0.55      0.48      0.42      6680\n",
      "\n",
      "\n",
      "GRU Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.37      0.93      0.53      1898\n",
      "     2 stars       0.00      0.00      0.00       645\n",
      "     3 stars       0.00      0.00      0.00       939\n",
      "     4 stars       0.84      0.24      0.38      1146\n",
      "     5 stars       0.63      0.48      0.54      2052\n",
      "\n",
      "    accuracy                           0.45      6680\n",
      "   macro avg       0.37      0.33      0.29      6680\n",
      "weighted avg       0.44      0.45      0.38      6680\n",
      "\n",
      "\n",
      "BiRNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      1 star       0.40      0.94      0.56      1898\n",
      "     2 stars       0.72      0.10      0.17       645\n",
      "     3 stars       0.33      0.02      0.04       939\n",
      "     4 stars       0.79      0.25      0.38      1146\n",
      "     5 stars       0.64      0.52      0.58      2052\n",
      "\n",
      "    accuracy                           0.48      6680\n",
      "   macro avg       0.58      0.37      0.34      6680\n",
      "weighted avg       0.56      0.48      0.42      6680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "models = {\n",
    "    'CNN': create_cnn_model(X_train.shape[1]),\n",
    "    'LSTM': create_lstm_model(X_train.shape[1]),\n",
    "    'GRU': create_gru_model(X_train.shape[1]),\n",
    "    'BiRNN': create_birnn_model(X_train.shape[1]),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Получаем индекс с максимальной вероятностью\n",
    "    # Преобразуем y_test обратно в метки, так как они были в one-hot encoding\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    results[model_name] = classification_report(y_test_classes, y_pred_classes, target_names=['1 star', '2 stars', '3 stars', '4 stars', '5 stars'])\n",
    "\n",
    "# Вывод метрик для каждой модели\n",
    "for model_name, report in results.items():\n",
    "    print(f\"\\n{model_name} Model Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = word2vec_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokens = data[\"tokens_no_stopwords\"]\n",
    "labels = data[\"rating\"] \n",
    "y = labels.map({'1 star': 0, '2 stars': 0, '3 stars': 0, '4 stars': 1, '5 stars': 1})\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=2)\n",
    "\n",
    "X = vectors\n",
    "\n",
    "# Разделение данных\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Dropout(0.5),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')  # 5 классов\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(2, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Определение модели GRU\n",
    "def create_gru_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.GRU(64, return_sequences=False),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(2, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Определение модели BiRNN\n",
    "def create_birnn_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.BatchNormalization(),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),  # Добавление Dropout для предотвращения переобучения\n",
    "        tf.keras.layers.Dense(2, activation='softmax')  # 5 классов для многоклассовой классификации\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN model...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 21ms/step - accuracy: 0.6947 - loss: 0.5641 - val_accuracy: 0.7400 - val_loss: 0.5234\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.7415 - loss: 0.5119 - val_accuracy: 0.7418 - val_loss: 0.5150\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.7414 - loss: 0.5089 - val_accuracy: 0.7403 - val_loss: 0.5116\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.7412 - loss: 0.5102 - val_accuracy: 0.7431 - val_loss: 0.5109\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 20ms/step - accuracy: 0.7469 - loss: 0.5079 - val_accuracy: 0.7458 - val_loss: 0.5127\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7448 - loss: 0.5123\n",
      "Test accuracy: 74.31%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Training LSTM model...\n",
      "Epoch 1/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 27ms/step - accuracy: 0.6118 - loss: 0.6586 - val_accuracy: 0.6662 - val_loss: 0.6517\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.6523 - loss: 0.6242 - val_accuracy: 0.6879 - val_loss: 0.5818\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 25ms/step - accuracy: 0.6799 - loss: 0.5858 - val_accuracy: 0.5647 - val_loss: 0.6090\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.6900 - loss: 0.5806 - val_accuracy: 0.7165 - val_loss: 0.5655\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.7009 - loss: 0.5682 - val_accuracy: 0.7136 - val_loss: 0.5591\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7076 - loss: 0.5606\n",
      "Test accuracy: 71.36%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "Training GRU model...\n",
      "Epoch 1/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 28ms/step - accuracy: 0.5961 - loss: 0.6679 - val_accuracy: 0.4844 - val_loss: 0.6641\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.6409 - loss: 0.6379 - val_accuracy: 0.6647 - val_loss: 0.6112\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.6642 - loss: 0.6101 - val_accuracy: 0.6734 - val_loss: 0.6044\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 26ms/step - accuracy: 0.6960 - loss: 0.5774 - val_accuracy: 0.7150 - val_loss: 0.5672\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 27ms/step - accuracy: 0.7074 - loss: 0.5630 - val_accuracy: 0.7151 - val_loss: 0.5520\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7134 - loss: 0.5553\n",
      "Test accuracy: 71.51%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "Training BiRNN model...\n",
      "Epoch 1/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 34ms/step - accuracy: 0.6128 - loss: 0.6618 - val_accuracy: 0.7085 - val_loss: 0.6035\n",
      "Epoch 2/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 48ms/step - accuracy: 0.6974 - loss: 0.5797 - val_accuracy: 0.7145 - val_loss: 0.5756\n",
      "Epoch 3/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 36ms/step - accuracy: 0.7076 - loss: 0.5640 - val_accuracy: 0.7172 - val_loss: 0.5456\n",
      "Epoch 4/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.7133 - loss: 0.5557 - val_accuracy: 0.7225 - val_loss: 0.5451\n",
      "Epoch 5/5\n",
      "\u001b[1m418/418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 33ms/step - accuracy: 0.7181 - loss: 0.5456 - val_accuracy: 0.7225 - val_loss: 0.5444\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7228 - loss: 0.5440\n",
      "Test accuracy: 72.25%\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step\n",
      "\n",
      "CNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.69      0.91      0.79      3482\n",
      "    Positive       0.85      0.56      0.68      3198\n",
      "\n",
      "    accuracy                           0.74      6680\n",
      "   macro avg       0.77      0.74      0.73      6680\n",
      "weighted avg       0.77      0.74      0.73      6680\n",
      "\n",
      "\n",
      "LSTM Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.66      0.91      0.77      3482\n",
      "    Positive       0.84      0.50      0.63      3198\n",
      "\n",
      "    accuracy                           0.71      6680\n",
      "   macro avg       0.75      0.70      0.70      6680\n",
      "weighted avg       0.75      0.71      0.70      6680\n",
      "\n",
      "\n",
      "GRU Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.67      0.89      0.77      3482\n",
      "    Positive       0.82      0.52      0.64      3198\n",
      "\n",
      "    accuracy                           0.72      6680\n",
      "   macro avg       0.74      0.71      0.70      6680\n",
      "weighted avg       0.74      0.72      0.70      6680\n",
      "\n",
      "\n",
      "BiRNN Model Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.68      0.89      0.77      3482\n",
      "    Positive       0.82      0.54      0.65      3198\n",
      "\n",
      "    accuracy                           0.72      6680\n",
      "   macro avg       0.75      0.71      0.71      6680\n",
      "weighted avg       0.75      0.72      0.71      6680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "models = {\n",
    "    'CNN': create_cnn_model(X_train.shape[1]),\n",
    "    'LSTM': create_lstm_model(X_train.shape[1]),\n",
    "    'GRU': create_gru_model(X_train.shape[1]),\n",
    "    'BiRNN': create_birnn_model(X_train.shape[1]),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name} model...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f'Test accuracy: {accuracy*100:.2f}%')\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)  # Получаем индекс с максимальной вероятностью\n",
    "    # Преобразуем y_test обратно в метки, так как они были в one-hot encoding\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "    results[model_name] = classification_report(y_test_classes, y_pred_classes, target_names=['Negative', 'Positive'])\n",
    "\n",
    "# Вывод метрик для каждой модели\n",
    "for model_name, report in results.items():\n",
    "    print(f\"\\n{model_name} Model Classification Report:\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokens = data[\"tokens_no_stopwords\"]\n",
    "labels = data[\"rating\"] \n",
    "y = labels.map({'1 star': 0, '2 stars': 0, '3 stars': 0, '4 stars': 1, '5 stars': 1})\n",
    "y  = tf.keras.utils.to_categorical(y , num_classes=2)\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(data[\"cleaned_review\"])\n",
    "sequences = tokenizer.texts_to_sequences(data[\"cleaned_review\"])\n",
    "\n",
    "X = pad_sequences(sequences, padding='post', maxlen=50)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Attention\n",
    "\n",
    "def create_gru_model(input_shape, num_classes=2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.GRU(64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes=2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_birnn_model(input_shape, num_classes=2):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=input_shape),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_seq2seq_model(input_shape, num_classes=2):\n",
    "    # Encoder\n",
    "    encoder_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=128)(encoder_input)\n",
    "    encoder_output, state_h, state_c = tf.keras.layers.LSTM(64, return_state=True)(embedding)\n",
    "\n",
    "    # Attention\n",
    "    attention = tf.keras.layers.Attention()([encoder_output, embedding])\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = tf.keras.layers.RepeatVector(1)(attention)\n",
    "    decoder_output = tf.keras.layers.LSTM(64, return_sequences=False)(decoder_input)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(decoder_output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=encoder_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_seq2seq_rnn_model(input_shape, num_classes=2):\n",
    "    # Seq2Seq Encoder\n",
    "    encoder_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=128)(encoder_input)\n",
    "    encoder_output, state_h, state_c = tf.keras.layers.LSTM(64, return_state=True)(embedding)\n",
    "    \n",
    "    # RNN Classifier\n",
    "    rnn_input = tf.keras.layers.RepeatVector(1)(encoder_output)\n",
    "    rnn_output = tf.keras.layers.LSTM(64, return_sequences=False)(rnn_input)\n",
    "    dense_output = tf.keras.layers.Dense(64, activation='relu')(rnn_output)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_output)\n",
    "    \n",
    "    # Model definition\n",
    "    model = tf.keras.Model(inputs=encoder_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def create_seq2seq_birnn_model(input_shape, num_classes=2):\n",
    "    # Seq2Seq Encoder\n",
    "    encoder_input = tf.keras.layers.Input(shape=(input_shape,))\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=5000, output_dim=128)(encoder_input)\n",
    "    encoder_output, state_h, state_c = tf.keras.layers.LSTM(64, return_state=True)(embedding)\n",
    "    \n",
    "    # BiRNN Classifier\n",
    "    birnn_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(64, return_sequences=False)\n",
    "    )(tf.keras.layers.RepeatVector(1)(encoder_output))\n",
    "    dense_output = tf.keras.layers.Dense(64, activation='relu')(birnn_output)\n",
    "    output = tf.keras.layers.Dense(num_classes, activation='softmax')(dense_output)\n",
    "    \n",
    "    # Model definition\n",
    "    model = tf.keras.Model(inputs=encoder_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GRU...\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marat/.local/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 13ms/step - accuracy: 0.6161 - loss: 0.6281 - val_accuracy: 0.8395 - val_loss: 0.3770\n",
      "Epoch 2/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8644 - loss: 0.3364 - val_accuracy: 0.8636 - val_loss: 0.3104\n",
      "Epoch 3/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8974 - loss: 0.2575 - val_accuracy: 0.8707 - val_loss: 0.2951\n",
      "Epoch 4/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9149 - loss: 0.2143 - val_accuracy: 0.8757 - val_loss: 0.3164\n",
      "Epoch 5/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9293 - loss: 0.1842 - val_accuracy: 0.8734 - val_loss: 0.3297\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.86      0.88      3482\n",
      "    Positive       0.85      0.89      0.87      3198\n",
      "\n",
      "    accuracy                           0.87      6680\n",
      "   macro avg       0.87      0.87      0.87      6680\n",
      "weighted avg       0.87      0.87      0.87      6680\n",
      "\n",
      "Training LSTM...\n",
      "Epoch 1/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.6235 - loss: 0.6441 - val_accuracy: 0.8229 - val_loss: 0.4439\n",
      "Epoch 2/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8578 - loss: 0.3635 - val_accuracy: 0.8716 - val_loss: 0.3192\n",
      "Epoch 3/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9105 - loss: 0.2390 - val_accuracy: 0.8784 - val_loss: 0.2948\n",
      "Epoch 4/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9254 - loss: 0.2030 - val_accuracy: 0.8804 - val_loss: 0.3090\n",
      "Epoch 5/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9358 - loss: 0.1725 - val_accuracy: 0.8772 - val_loss: 0.3315\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.88      0.88      3482\n",
      "    Positive       0.87      0.88      0.87      3198\n",
      "\n",
      "    accuracy                           0.88      6680\n",
      "   macro avg       0.88      0.88      0.88      6680\n",
      "weighted avg       0.88      0.88      0.88      6680\n",
      "\n",
      "Training BiRNN...\n",
      "Epoch 1/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 14ms/step - accuracy: 0.7933 - loss: 0.4206 - val_accuracy: 0.8754 - val_loss: 0.2990\n",
      "Epoch 2/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 19ms/step - accuracy: 0.9040 - loss: 0.2426 - val_accuracy: 0.8786 - val_loss: 0.2859\n",
      "Epoch 3/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.9216 - loss: 0.2014 - val_accuracy: 0.8811 - val_loss: 0.2961\n",
      "Epoch 4/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9300 - loss: 0.1693 - val_accuracy: 0.8828 - val_loss: 0.3307\n",
      "Epoch 5/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 14ms/step - accuracy: 0.9436 - loss: 0.1450 - val_accuracy: 0.8780 - val_loss: 0.3528\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.86      0.88      3482\n",
      "    Positive       0.85      0.90      0.88      3198\n",
      "\n",
      "    accuracy                           0.88      6680\n",
      "   macro avg       0.88      0.88      0.88      6680\n",
      "weighted avg       0.88      0.88      0.88      6680\n",
      "\n",
      "Training Seq2Seq+RNN...\n",
      "Epoch 1/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.6628 - loss: 0.6265 - val_accuracy: 0.7413 - val_loss: 0.5593\n",
      "Epoch 2/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.7829 - loss: 0.4771 - val_accuracy: 0.8735 - val_loss: 0.3059\n",
      "Epoch 3/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9023 - loss: 0.2514 - val_accuracy: 0.8757 - val_loss: 0.2951\n",
      "Epoch 4/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9207 - loss: 0.2029 - val_accuracy: 0.8792 - val_loss: 0.3065\n",
      "Epoch 5/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9339 - loss: 0.1715 - val_accuracy: 0.8825 - val_loss: 0.3243\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.89      0.88      0.89      3482\n",
      "    Positive       0.87      0.88      0.88      3198\n",
      "\n",
      "    accuracy                           0.88      6680\n",
      "   macro avg       0.88      0.88      0.88      6680\n",
      "weighted avg       0.88      0.88      0.88      6680\n",
      "\n",
      "Training Seq2Seq+BiRNN...\n",
      "Epoch 1/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13ms/step - accuracy: 0.6087 - loss: 0.6494 - val_accuracy: 0.7850 - val_loss: 0.5055\n",
      "Epoch 2/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.8280 - loss: 0.4270 - val_accuracy: 0.8563 - val_loss: 0.3403\n",
      "Epoch 3/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 15ms/step - accuracy: 0.8889 - loss: 0.2712 - val_accuracy: 0.8744 - val_loss: 0.3137\n",
      "Epoch 4/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9128 - loss: 0.2187 - val_accuracy: 0.8722 - val_loss: 0.3235\n",
      "Epoch 5/5\n",
      "\u001b[1m835/835\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 13ms/step - accuracy: 0.9282 - loss: 0.1828 - val_accuracy: 0.8786 - val_loss: 0.3209\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.91      0.85      0.88      3482\n",
      "    Positive       0.85      0.91      0.88      3198\n",
      "\n",
      "    accuracy                           0.88      6680\n",
      "   macro avg       0.88      0.88      0.88      6680\n",
      "weighted avg       0.88      0.88      0.88      6680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Модели для обучения\n",
    "models = {\n",
    "    \"GRU\": create_gru_model(input_shape=X_train.shape[1]),\n",
    "    \"LSTM\": create_lstm_model(input_shape=X_train.shape[1]),\n",
    "    \"BiRNN\": create_birnn_model(input_shape=X_train.shape[1]),\n",
    "    \"Seq2Seq+RNN\": create_seq2seq_rnn_model(input_shape=X_train.shape[1]),\n",
    "    \"Seq2Seq+BiRNN\": create_seq2seq_birnn_model(input_shape=X_train.shape[1]),\n",
    "}\n",
    "\n",
    "# Обучение и оценка моделей\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    model.fit(X_train, y_train, epochs=5, batch_size=32, validation_data=(X_test, y_test))\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "    results[model_name] = classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"])\n",
    "    print(results[model_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(models, X):\n",
    "    # Получаем предсказания всех моделей\n",
    "    predictions = [model.predict(X) for model in models.values()]\n",
    "    # Усредняем вероятности\n",
    "    avg_prediction = np.mean(predictions, axis=0)\n",
    "    return np.argmax(avg_prediction, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m209/209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.88      0.89      3482\n",
      "    Positive       0.87      0.89      0.88      3198\n",
      "\n",
      "    accuracy                           0.89      6680\n",
      "   macro avg       0.89      0.89      0.89      6680\n",
      "weighted avg       0.89      0.89      0.89      6680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = ensemble_predict(models, X_test)\n",
    "\n",
    "# Оценка ансамбля\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fd94a9e98741328333a37fb54123fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26716 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba006563f349f38f0b44d386b13ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_276164/2080335055.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23' max='5010' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  23/5010 03:30 < 13:54:43, 0.10 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 85\u001b[0m\n\u001b[1;32m     75\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     76\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     77\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     81\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m     82\u001b[0m )\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# Обучение модели\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# Оценка модели\u001b[39;00m\n\u001b[1;32m     88\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2171\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2169\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2172\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:2531\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2524\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2525\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2526\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2527\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2529\u001b[0m )\n\u001b[1;32m   2530\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2531\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2534\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2535\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2536\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2537\u001b[0m ):\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2539\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/trainer.py:3715\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3713\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3715\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3717\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/accelerate/accelerator.py:2248\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2247\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2248\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Убедимся, что используем только CPU\n",
    "torch.device(\"cpu\")\n",
    "\n",
    "# Загружаем данные\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Функция для предобработки данных.\"\"\"\n",
    "    labels = data[\"rating\"].map({'1 star': 0, '2 stars': 1, '3 stars': 2, '4 stars': 3, '5 stars': 4})\n",
    "    return {\n",
    "        \"text\": data[\"cleaned_review\"],\n",
    "        \"label\": labels,\n",
    "    }\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Убедимся, что данные представлены в виде pandas DataFrame\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# Преобразуем данные в Dataset\n",
    "train_data = preprocess_data(train_df)\n",
    "test_data = preprocess_data(test_df)\n",
    "\n",
    "# Создание датасета для Hugging Face\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_data),\n",
    "    \"test\": Dataset.from_dict(test_data),\n",
    "})\n",
    "\n",
    "# Загружаем токенайзер и модель\n",
    "model_name = \"distilbert-base-uncased\"  # Можно заменить на другую трансформерную модель\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
    "\n",
    "# Устанавливаем модель в режим CPU\n",
    "model.to(\"cpu\")\n",
    "\n",
    "# Токенизация данных\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"text\"])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")\n",
    "\n",
    "# Определение вычисления метрик\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = predictions.argmax(axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "# Аргументы тренировки\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    use_cpu=True,  # Отключаем CUDA\n",
    ")\n",
    "\n",
    "# Создаем объект Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Обучение модели\n",
    "trainer.train()\n",
    "\n",
    "# Оценка модели\n",
    "results = trainer.evaluate()\n",
    "print(results)\n",
    "\n",
    "# Генерация отчета классификации\n",
    "predictions = trainer.predict(tokenized_dataset[\"test\"])\n",
    "pred_labels = predictions.predictions.argmax(axis=1)\n",
    "true_labels = tokenized_dataset[\"test\"][\"labels\"]\n",
    "\n",
    "print(classification_report(true_labels, pred_labels, target_names=[\"1 star\", \"2 stars\", \"3 stars\", \"4 stars\", \"5 stars\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa72c7f945b44de8a4313b2c9041991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe2923f3104be2b3bcfccd09397160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbeb7845dee94027b461942c66cabae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "477d743dce3648ae9067ce27c687ff2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998855590820312}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I love this product!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer_id</th>\n",
       "      <th>store_name</th>\n",
       "      <th>category</th>\n",
       "      <th>store_address</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>rating_count</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>3 months ago</td>\n",
       "      <td>Why does it look like someone spit on my food?...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>why does it look like someone spit on my food ...</td>\n",
       "      <td>[why, does, it, look, like, someone, spit, on,...</td>\n",
       "      <td>[look, like, someone, spit, food, normal, tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>It'd McDonalds. It is what it is as far as the...</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>itd mcdonalds it is what it is as far as the f...</td>\n",
       "      <td>[itd, mcdonalds, it, is, what, it, is, as, far...</td>\n",
       "      <td>[itd, mcdonalds, far, food, atmosphere, go, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>Made a mobile order got to the speaker and che...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>made a mobile order got to the speaker and che...</td>\n",
       "      <td>[made, a, mobile, order, got, to, the, speaker...</td>\n",
       "      <td>[made, mobile, order, got, speaker, checked, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>a month ago</td>\n",
       "      <td>My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>my mc crispy chicken sandwich was customer ser...</td>\n",
       "      <td>[my, mc, crispy, chicken, sandwich, was, custo...</td>\n",
       "      <td>[mc, crispy, chicken, sandwich, customer, serv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>13749 US-183 Hwy, Austin, TX 78750, United States</td>\n",
       "      <td>30.460718</td>\n",
       "      <td>-97.792874</td>\n",
       "      <td>1,240</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>I repeat my order 3 times in the drive thru, a...</td>\n",
       "      <td>1 star</td>\n",
       "      <td>i repeat my order 3 times in the drive thru an...</td>\n",
       "      <td>[i, repeat, my, order, 3, times, in, the, driv...</td>\n",
       "      <td>[repeat, order, 3, times, drive, thru, still, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33391</th>\n",
       "      <td>33392</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>They treated me very badly.</td>\n",
       "      <td>1 star</td>\n",
       "      <td>they treated me very badly</td>\n",
       "      <td>[they, treated, me, very, badly]</td>\n",
       "      <td>[treated, badly]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33392</th>\n",
       "      <td>33393</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>The service is very good</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>the service is very good</td>\n",
       "      <td>[the, service, is, very, good]</td>\n",
       "      <td>[service, good]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33393</th>\n",
       "      <td>33394</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>a year ago</td>\n",
       "      <td>To remove hunger is enough</td>\n",
       "      <td>4 stars</td>\n",
       "      <td>to remove hunger is enough</td>\n",
       "      <td>[to, remove, hunger, is, enough]</td>\n",
       "      <td>[remove, hunger, enough]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33394</th>\n",
       "      <td>33395</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>5 years ago</td>\n",
       "      <td>It's good, but lately it has become very expen...</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>its good but lately it has become very expensive</td>\n",
       "      <td>[its, good, but, lately, it, has, become, very...</td>\n",
       "      <td>[good, lately, become, expensive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33395</th>\n",
       "      <td>33396</td>\n",
       "      <td>McDonald's</td>\n",
       "      <td>Fast food restaurant</td>\n",
       "      <td>3501 Biscayne Blvd, Miami, FL 33137, United St...</td>\n",
       "      <td>25.810000</td>\n",
       "      <td>-80.189098</td>\n",
       "      <td>2,810</td>\n",
       "      <td>2 years ago</td>\n",
       "      <td>they took good care of me</td>\n",
       "      <td>5 stars</td>\n",
       "      <td>they took good care of me</td>\n",
       "      <td>[they, took, good, care, of, me]</td>\n",
       "      <td>[took, good, care]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33396 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewer_id  store_name              category  \\\n",
       "0                1  McDonald's  Fast food restaurant   \n",
       "1                2  McDonald's  Fast food restaurant   \n",
       "2                3  McDonald's  Fast food restaurant   \n",
       "3                4  McDonald's  Fast food restaurant   \n",
       "4                5  McDonald's  Fast food restaurant   \n",
       "...            ...         ...                   ...   \n",
       "33391        33392  McDonald's  Fast food restaurant   \n",
       "33392        33393  McDonald's  Fast food restaurant   \n",
       "33393        33394  McDonald's  Fast food restaurant   \n",
       "33394        33395  McDonald's  Fast food restaurant   \n",
       "33395        33396  McDonald's  Fast food restaurant   \n",
       "\n",
       "                                           store_address  latitude   \\\n",
       "0      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "1      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "2      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "3      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "4      13749 US-183 Hwy, Austin, TX 78750, United States  30.460718   \n",
       "...                                                  ...        ...   \n",
       "33391  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33392  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33393  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33394  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "33395  3501 Biscayne Blvd, Miami, FL 33137, United St...  25.810000   \n",
       "\n",
       "       longitude rating_count   review_time  \\\n",
       "0     -97.792874        1,240  3 months ago   \n",
       "1     -97.792874        1,240    5 days ago   \n",
       "2     -97.792874        1,240    5 days ago   \n",
       "3     -97.792874        1,240   a month ago   \n",
       "4     -97.792874        1,240  2 months ago   \n",
       "...          ...          ...           ...   \n",
       "33391 -80.189098        2,810   4 years ago   \n",
       "33392 -80.189098        2,810    a year ago   \n",
       "33393 -80.189098        2,810    a year ago   \n",
       "33394 -80.189098        2,810   5 years ago   \n",
       "33395 -80.189098        2,810   2 years ago   \n",
       "\n",
       "                                                  review   rating  \\\n",
       "0      Why does it look like someone spit on my food?...   1 star   \n",
       "1      It'd McDonalds. It is what it is as far as the...  4 stars   \n",
       "2      Made a mobile order got to the speaker and che...   1 star   \n",
       "3      My mc. Crispy chicken sandwich was ï¿½ï¿½ï¿½ï¿...  5 stars   \n",
       "4      I repeat my order 3 times in the drive thru, a...   1 star   \n",
       "...                                                  ...      ...   \n",
       "33391                        They treated me very badly.   1 star   \n",
       "33392                           The service is very good  5 stars   \n",
       "33393                         To remove hunger is enough  4 stars   \n",
       "33394  It's good, but lately it has become very expen...  5 stars   \n",
       "33395                          they took good care of me  5 stars   \n",
       "\n",
       "                                          cleaned_review  \\\n",
       "0      why does it look like someone spit on my food ...   \n",
       "1      itd mcdonalds it is what it is as far as the f...   \n",
       "2      made a mobile order got to the speaker and che...   \n",
       "3      my mc crispy chicken sandwich was customer ser...   \n",
       "4      i repeat my order 3 times in the drive thru an...   \n",
       "...                                                  ...   \n",
       "33391                         they treated me very badly   \n",
       "33392                           the service is very good   \n",
       "33393                         to remove hunger is enough   \n",
       "33394   its good but lately it has become very expensive   \n",
       "33395                          they took good care of me   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [why, does, it, look, like, someone, spit, on,...   \n",
       "1      [itd, mcdonalds, it, is, what, it, is, as, far...   \n",
       "2      [made, a, mobile, order, got, to, the, speaker...   \n",
       "3      [my, mc, crispy, chicken, sandwich, was, custo...   \n",
       "4      [i, repeat, my, order, 3, times, in, the, driv...   \n",
       "...                                                  ...   \n",
       "33391                   [they, treated, me, very, badly]   \n",
       "33392                     [the, service, is, very, good]   \n",
       "33393                   [to, remove, hunger, is, enough]   \n",
       "33394  [its, good, but, lately, it, has, become, very...   \n",
       "33395                   [they, took, good, care, of, me]   \n",
       "\n",
       "                                     tokens_no_stopwords  \n",
       "0      [look, like, someone, spit, food, normal, tran...  \n",
       "1      [itd, mcdonalds, far, food, atmosphere, go, st...  \n",
       "2      [made, mobile, order, got, speaker, checked, l...  \n",
       "3      [mc, crispy, chicken, sandwich, customer, serv...  \n",
       "4      [repeat, order, 3, times, drive, thru, still, ...  \n",
       "...                                                  ...  \n",
       "33391                                   [treated, badly]  \n",
       "33392                                    [service, good]  \n",
       "33393                           [remove, hunger, enough]  \n",
       "33394                  [good, lately, become, expensive]  \n",
       "33395                                 [took, good, care]  \n",
       "\n",
       "[33396 rows x 13 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m texts \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Выполняем анализ тональности\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, sentiment \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(texts, sentiments):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSentiment: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Confidence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1362\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1355\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1356\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         )\n\u001b[1;32m   1360\u001b[0m     )\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1368\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1368\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1369\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1370\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:183\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2928\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2925\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2928\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2929\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2930\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2931\u001b[0m     )\n\u001b[1;32m   2933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2934\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2935\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2937\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Загружаем предобученную модель для анализа тональности\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Пример текста для анализа тональности\n",
    "texts = data[\"review\"]\n",
    "\n",
    "# Выполняем анализ тональности\n",
    "sentiments = sentiment_analysis(texts)\n",
    "for text, sentiment in zip(texts, sentiments):\n",
    "    print(f\"Text: {text}\\nSentiment: {sentiment['label']}, Confidence: {sentiment['score']}\\n\")\n",
    "\n",
    "# Задача классификации текста с использованием модели для классификации\n",
    "# Загрузим набор данных для классификации\n",
    "dataset = load_dataset(\"ag_news\", split='test[:10%]')\n",
    "\n",
    "# Загружаем модель для классификации текста\n",
    "text_classification = pipeline(\"zero-shot-classification\")\n",
    "\n",
    "# Применим классификацию к некоторым примерам из набора данных\n",
    "labels = [\"World\", \"Sports\", \"Business\", \"Science/Technology\"]\n",
    "for text in dataset['text'][:5]:  # Пример с 5 текстами\n",
    "    classification = text_classification(text, candidate_labels=labels)\n",
    "    print(f\"Text: {text}\\nPredicted Label: {classification['labels'][0]}, Score: {classification['scores'][0]}\\n\")\n",
    "\n",
    "# Если необходимо вычислить метрики\n",
    "true_labels = [item['label'] for item in dataset[:5]]\n",
    "predicted_labels = [text_classification(text, candidate_labels=labels)['labels'][0] for text in dataset['text'][:5]]\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m sentiment_analysis \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Применяем анализ тональности к отзывам\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_review\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Добавляем результаты анализа тональности в DataFrame\u001b[39;00m\n\u001b[1;32m     22\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_sentiment\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/base.py:1343\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1340\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1341\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1342\u001b[0m     )\n\u001b[0;32m-> 1343\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1345\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:19\u001b[0m, in \u001b[0;36mPipelineDataset.__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     18\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[i]\n\u001b[0;32m---> 19\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m processed\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/pipelines/text_classification.py:183\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# This is likely an invalid usage of the pipeline attempting to pass text pairs.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pipeline received invalid inputs, if you are trying to send text pairs, you can try to send a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m dictionary `\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy text\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMy pair\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m}` in order to send a text pair.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2868\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2866\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2867\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2868\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2978\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2956\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m   2957\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[1;32m   2958\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2975\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2976\u001b[0m     )\n\u001b[1;32m   2977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2997\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2999\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3054\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3045\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3046\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3047\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3051\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3052\u001b[0m )\n\u001b[0;32m-> 3054\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3056\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3057\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3058\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3059\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3060\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3073\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3074\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3075\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:613\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    591\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    611\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    612\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 613\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/tokenization_utils_fast.py:539\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m!=\u001b[39m split_special_tokens:\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_special_tokens \u001b[38;5;241m=\u001b[39m split_special_tokens\n\u001b[0;32m--> 539\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_pretokenized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    553\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    563\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def map_rating_to_sentiment(rating):\n",
    "    if \"1 star\" in rating:\n",
    "        return \"negative\"\n",
    "    elif \"4 stars\" in rating:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "data['sentiment'] = data['rating'].apply(map_rating_to_sentiment)\n",
    "\n",
    "# Загрузка предобученной модели для анализа тональности\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Применяем анализ тональности к отзывам\n",
    "results = sentiment_analysis(data['cleaned_review'].tolist())\n",
    "\n",
    "# Добавляем результаты анализа тональности в DataFrame\n",
    "data['predicted_sentiment'] = [result['label'] for result in results]\n",
    "data['predicted_score'] = [result['score'] for result in results]\n",
    "\n",
    "# Сравнение предсказанных и истинных значений тональности\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(data['sentiment'], data['predicted_sentiment']))\n",
    "\n",
    "# Выводим результаты\n",
    "print(data[['review', 'sentiment', 'predicted_sentiment', 'predicted_score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
